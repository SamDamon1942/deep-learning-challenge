


# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from google.colab import files #need this to download files

#  Import and read the charity_data.csv.
application_df = pd.read_csv("https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv")
application_df.head()





# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
application_df.drop(["EIN", "NAME"], axis=1,inplace=True)
application_df.head()



# Determine the number of unique values in each column.
unique_counts = application_df.nunique()
print(unique_counts)


# For columns that have more than 10 unique values, determine the number of data points for each unique value.
# Based on the results above, there are three columns with more than 10 unique values:
# APPLICATION_TYPE
# CLASSIFICATION
# ASK_AMT
value_counts_application_type = application_df["APPLICATION_TYPE"].value_counts()
value_counts_classification = application_df["CLASSIFICATION"].value_counts()
value_counts_ask_amt = application_df["ASK_AMT"].value_counts()




#Look at "APPLICATION_TYPE" value counts to identify and replace with "Other"
print(value_counts_application_type)


# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace`
application_types_to_replace = ['T9','T13','T12','T2','T14','T25','T15','T29','T17']

# Replace in dataframe
for app in application_types_to_replace:
    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,"Other")

# Check to make sure replacement was successful
application_df['APPLICATION_TYPE'].value_counts()


#Look at "CLASSIFICATION" value counts to identify and replace with "Other"
print(value_counts_classification)


# Look at CLASSIFICATION value counts >1
value_counts_classification_greater_than_1 = value_counts_classification[value_counts_classification>1]
print(value_counts_classification_greater_than_1)


#copied_df= application_df.copy() make a copy so if my code doesn't work, I don't need to start from the beginning!
values_to_replace = application_df["CLASSIFICATION"].value_counts().index[-66:]  #instead of listing 66 classifications, choose the 66 classifications with the lowest counts
application_df["CLASSIFICATION"]=application_df["CLASSIFICATION"].replace(values_to_replace,"Other")
application_df['CLASSIFICATION'].value_counts()


# Convert categorical data to numeric with `pd.get_dummies`
copied_df= application_df.copy()
application_df_new = pd.get_dummies(application_df)
application_df_new.head()


#use the following code to confirm the features that are used in the model
unique_counts_new = application_df_new.nunique()
print(unique_counts_new)


# Split our preprocessed data into our features and target arrays
y=application_df_new['IS_SUCCESSFUL']
X=application_df_new.drop(columns='IS_SUCCESSFUL')

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)


# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)





# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.

nn_model = tf.keras.models.Sequential()

# First hidden layer
nn_model.add(tf.keras.layers.Dense(units=80, activation="relu", input_dim=43))

# Second hidden layer
nn_model.add(tf.keras.layers.Dense(units=30, activation="relu"))

# Output layer
nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the model
nn_model.summary()


#create a model checkpoint callback to save the model's weights every 5 epochs

# Define the filepath where the model weights will be saved
filepath = "model_weights_Alphabet_Soup_Charity.h5"

# Create a ModelCheckpoint callback to save the model weights every 5 epochs
checkpoint_callback = ModelCheckpoint(filepath,
                                      save_weights_only=True,
                                      save_freq='epoch',
                                      period=5)



# Compile the model
nn_model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])


# Train the model
fit_model = nn_model.fit(X_train_scaled, y_train, epochs=100, callbacks=[checkpoint_callback])


# Evaluate the model using the test data
model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


# Export our model to HDF5 file
nn_model.save('AphabetSoupCharity.h5')



files.download('AphabetSoupCharity.h5')


files.download('model_weights_Alphabet_Soup_Charity.h5')
